{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)       [(None, 256, 3072)]          0         []                            \n",
      "                                                                                                  \n",
      " dense_700 (Dense)           (None, 256, 256)             786688    ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (T  (None, 256, 256)             0         ['dense_700[0][0]']           \n",
      " FOpLambda)                                                                                       \n",
      "                                                                                                  \n",
      " class_token_40 (ClassToken  (None, 1, 256)               256       ['tf.__operators__.add_19[0][0\n",
      " )                                                                  ]']                           \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenat  (None, 257, 256)             0         ['class_token_40[0][0]',      \n",
      " e)                                                                  'tf.__operators__.add_19[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_686 (L  (None, 257, 256)             512       ['concatenate_14[0][0]']      \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_336 (  (None, 257, 256)             3155200   ['layer_normalization_686[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_686[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_672 (Add)               (None, 257, 256)             0         ['multi_head_attention_336[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'concatenate_14[0][0]']      \n",
      "                                                                                                  \n",
      " layer_normalization_687 (L  (None, 257, 256)             512       ['add_672[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_701 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_687[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1296 (Dropout)      (None, 257, 1024)            0         ['dense_701[0][0]']           \n",
      "                                                                                                  \n",
      " dense_702 (Dense)           (None, 257, 256)             262400    ['dropout_1296[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1297 (Dropout)      (None, 257, 256)             0         ['dense_702[0][0]']           \n",
      "                                                                                                  \n",
      " add_673 (Add)               (None, 257, 256)             0         ['dropout_1297[0][0]',        \n",
      "                                                                     'add_672[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_688 (L  (None, 257, 256)             512       ['add_673[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_337 (  (None, 257, 256)             3155200   ['layer_normalization_688[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_688[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_674 (Add)               (None, 257, 256)             0         ['multi_head_attention_337[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_673[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_689 (L  (None, 257, 256)             512       ['add_674[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_703 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_689[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1298 (Dropout)      (None, 257, 1024)            0         ['dense_703[0][0]']           \n",
      "                                                                                                  \n",
      " dense_704 (Dense)           (None, 257, 256)             262400    ['dropout_1298[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1299 (Dropout)      (None, 257, 256)             0         ['dense_704[0][0]']           \n",
      "                                                                                                  \n",
      " add_675 (Add)               (None, 257, 256)             0         ['dropout_1299[0][0]',        \n",
      "                                                                     'add_674[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_690 (L  (None, 257, 256)             512       ['add_675[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_338 (  (None, 257, 256)             3155200   ['layer_normalization_690[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_690[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_676 (Add)               (None, 257, 256)             0         ['multi_head_attention_338[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_675[0][0]']             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer_normalization_691 (L  (None, 257, 256)             512       ['add_676[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_705 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_691[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1300 (Dropout)      (None, 257, 1024)            0         ['dense_705[0][0]']           \n",
      "                                                                                                  \n",
      " dense_706 (Dense)           (None, 257, 256)             262400    ['dropout_1300[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1301 (Dropout)      (None, 257, 256)             0         ['dense_706[0][0]']           \n",
      "                                                                                                  \n",
      " add_677 (Add)               (None, 257, 256)             0         ['dropout_1301[0][0]',        \n",
      "                                                                     'add_676[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_692 (L  (None, 257, 256)             512       ['add_677[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_339 (  (None, 257, 256)             3155200   ['layer_normalization_692[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_692[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_678 (Add)               (None, 257, 256)             0         ['multi_head_attention_339[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_677[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_693 (L  (None, 257, 256)             512       ['add_678[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_707 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_693[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1302 (Dropout)      (None, 257, 1024)            0         ['dense_707[0][0]']           \n",
      "                                                                                                  \n",
      " dense_708 (Dense)           (None, 257, 256)             262400    ['dropout_1302[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1303 (Dropout)      (None, 257, 256)             0         ['dense_708[0][0]']           \n",
      "                                                                                                  \n",
      " add_679 (Add)               (None, 257, 256)             0         ['dropout_1303[0][0]',        \n",
      "                                                                     'add_678[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_694 (L  (None, 257, 256)             512       ['add_679[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_340 (  (None, 257, 256)             3155200   ['layer_normalization_694[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_694[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_680 (Add)               (None, 257, 256)             0         ['multi_head_attention_340[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_679[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_695 (L  (None, 257, 256)             512       ['add_680[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_709 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_695[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1304 (Dropout)      (None, 257, 1024)            0         ['dense_709[0][0]']           \n",
      "                                                                                                  \n",
      " dense_710 (Dense)           (None, 257, 256)             262400    ['dropout_1304[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1305 (Dropout)      (None, 257, 256)             0         ['dense_710[0][0]']           \n",
      "                                                                                                  \n",
      " add_681 (Add)               (None, 257, 256)             0         ['dropout_1305[0][0]',        \n",
      "                                                                     'add_680[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_696 (L  (None, 257, 256)             512       ['add_681[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_341 (  (None, 257, 256)             3155200   ['layer_normalization_696[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_696[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_682 (Add)               (None, 257, 256)             0         ['multi_head_attention_341[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_681[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_697 (L  (None, 257, 256)             512       ['add_682[0][0]']             \n",
      " ayerNormalization)                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dense_711 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_697[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1306 (Dropout)      (None, 257, 1024)            0         ['dense_711[0][0]']           \n",
      "                                                                                                  \n",
      " dense_712 (Dense)           (None, 257, 256)             262400    ['dropout_1306[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1307 (Dropout)      (None, 257, 256)             0         ['dense_712[0][0]']           \n",
      "                                                                                                  \n",
      " add_683 (Add)               (None, 257, 256)             0         ['dropout_1307[0][0]',        \n",
      "                                                                     'add_682[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_698 (L  (None, 257, 256)             512       ['add_683[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_342 (  (None, 257, 256)             3155200   ['layer_normalization_698[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_698[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_684 (Add)               (None, 257, 256)             0         ['multi_head_attention_342[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_683[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_699 (L  (None, 257, 256)             512       ['add_684[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_713 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_699[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1308 (Dropout)      (None, 257, 1024)            0         ['dense_713[0][0]']           \n",
      "                                                                                                  \n",
      " dense_714 (Dense)           (None, 257, 256)             262400    ['dropout_1308[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1309 (Dropout)      (None, 257, 256)             0         ['dense_714[0][0]']           \n",
      "                                                                                                  \n",
      " add_685 (Add)               (None, 257, 256)             0         ['dropout_1309[0][0]',        \n",
      "                                                                     'add_684[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_700 (L  (None, 257, 256)             512       ['add_685[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_343 (  (None, 257, 256)             3155200   ['layer_normalization_700[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_700[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_686 (Add)               (None, 257, 256)             0         ['multi_head_attention_343[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_685[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_701 (L  (None, 257, 256)             512       ['add_686[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_715 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_701[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1310 (Dropout)      (None, 257, 1024)            0         ['dense_715[0][0]']           \n",
      "                                                                                                  \n",
      " dense_716 (Dense)           (None, 257, 256)             262400    ['dropout_1310[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1311 (Dropout)      (None, 257, 256)             0         ['dense_716[0][0]']           \n",
      "                                                                                                  \n",
      " add_687 (Add)               (None, 257, 256)             0         ['dropout_1311[0][0]',        \n",
      "                                                                     'add_686[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_702 (L  (None, 257, 256)             512       ['add_687[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_344 (  (None, 257, 256)             3155200   ['layer_normalization_702[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_702[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_688 (Add)               (None, 257, 256)             0         ['multi_head_attention_344[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_687[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_703 (L  (None, 257, 256)             512       ['add_688[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_717 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_703[0][0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1312 (Dropout)      (None, 257, 1024)            0         ['dense_717[0][0]']           \n",
      "                                                                                                  \n",
      " dense_718 (Dense)           (None, 257, 256)             262400    ['dropout_1312[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1313 (Dropout)      (None, 257, 256)             0         ['dense_718[0][0]']           \n",
      "                                                                                                  \n",
      " add_689 (Add)               (None, 257, 256)             0         ['dropout_1313[0][0]',        \n",
      "                                                                     'add_688[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_704 (L  (None, 257, 256)             512       ['add_689[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_345 (  (None, 257, 256)             3155200   ['layer_normalization_704[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_704[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_690 (Add)               (None, 257, 256)             0         ['multi_head_attention_345[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_689[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_705 (L  (None, 257, 256)             512       ['add_690[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_719 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_705[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1314 (Dropout)      (None, 257, 1024)            0         ['dense_719[0][0]']           \n",
      "                                                                                                  \n",
      " dense_720 (Dense)           (None, 257, 256)             262400    ['dropout_1314[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1315 (Dropout)      (None, 257, 256)             0         ['dense_720[0][0]']           \n",
      "                                                                                                  \n",
      " add_691 (Add)               (None, 257, 256)             0         ['dropout_1315[0][0]',        \n",
      "                                                                     'add_690[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_706 (L  (None, 257, 256)             512       ['add_691[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_346 (  (None, 257, 256)             3155200   ['layer_normalization_706[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_706[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_692 (Add)               (None, 257, 256)             0         ['multi_head_attention_346[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_691[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_707 (L  (None, 257, 256)             512       ['add_692[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_721 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_707[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1316 (Dropout)      (None, 257, 1024)            0         ['dense_721[0][0]']           \n",
      "                                                                                                  \n",
      " dense_722 (Dense)           (None, 257, 256)             262400    ['dropout_1316[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1317 (Dropout)      (None, 257, 256)             0         ['dense_722[0][0]']           \n",
      "                                                                                                  \n",
      " add_693 (Add)               (None, 257, 256)             0         ['dropout_1317[0][0]',        \n",
      "                                                                     'add_692[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_708 (L  (None, 257, 256)             512       ['add_693[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_347 (  (None, 257, 256)             3155200   ['layer_normalization_708[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_708[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_694 (Add)               (None, 257, 256)             0         ['multi_head_attention_347[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_693[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_709 (L  (None, 257, 256)             512       ['add_694[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_723 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_709[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_1318 (Dropout)      (None, 257, 1024)            0         ['dense_723[0][0]']           \n",
      "                                                                                                  \n",
      " dense_724 (Dense)           (None, 257, 256)             262400    ['dropout_1318[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1319 (Dropout)      (None, 257, 256)             0         ['dense_724[0][0]']           \n",
      "                                                                                                  \n",
      " add_695 (Add)               (None, 257, 256)             0         ['dropout_1319[0][0]',        \n",
      "                                                                     'add_694[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_710 (L  (None, 257, 256)             512       ['add_695[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_348 (  (None, 257, 256)             3155200   ['layer_normalization_710[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_710[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_696 (Add)               (None, 257, 256)             0         ['multi_head_attention_348[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_695[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_711 (L  (None, 257, 256)             512       ['add_696[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_725 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_711[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1320 (Dropout)      (None, 257, 1024)            0         ['dense_725[0][0]']           \n",
      "                                                                                                  \n",
      " dense_726 (Dense)           (None, 257, 256)             262400    ['dropout_1320[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1321 (Dropout)      (None, 257, 256)             0         ['dense_726[0][0]']           \n",
      "                                                                                                  \n",
      " add_697 (Add)               (None, 257, 256)             0         ['dropout_1321[0][0]',        \n",
      "                                                                     'add_696[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_712 (L  (None, 257, 256)             512       ['add_697[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_349 (  (None, 257, 256)             3155200   ['layer_normalization_712[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_712[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_698 (Add)               (None, 257, 256)             0         ['multi_head_attention_349[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_697[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_713 (L  (None, 257, 256)             512       ['add_698[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_727 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_713[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1322 (Dropout)      (None, 257, 1024)            0         ['dense_727[0][0]']           \n",
      "                                                                                                  \n",
      " dense_728 (Dense)           (None, 257, 256)             262400    ['dropout_1322[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1323 (Dropout)      (None, 257, 256)             0         ['dense_728[0][0]']           \n",
      "                                                                                                  \n",
      " add_699 (Add)               (None, 257, 256)             0         ['dropout_1323[0][0]',        \n",
      "                                                                     'add_698[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_714 (L  (None, 257, 256)             512       ['add_699[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_350 (  (None, 257, 256)             3155200   ['layer_normalization_714[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_714[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_700 (Add)               (None, 257, 256)             0         ['multi_head_attention_350[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_699[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_715 (L  (None, 257, 256)             512       ['add_700[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_729 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_715[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1324 (Dropout)      (None, 257, 1024)            0         ['dense_729[0][0]']           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_730 (Dense)           (None, 257, 256)             262400    ['dropout_1324[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1325 (Dropout)      (None, 257, 256)             0         ['dense_730[0][0]']           \n",
      "                                                                                                  \n",
      " add_701 (Add)               (None, 257, 256)             0         ['dropout_1325[0][0]',        \n",
      "                                                                     'add_700[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_716 (L  (None, 257, 256)             512       ['add_701[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_351 (  (None, 257, 256)             3155200   ['layer_normalization_716[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_716[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_702 (Add)               (None, 257, 256)             0         ['multi_head_attention_351[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_701[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_717 (L  (None, 257, 256)             512       ['add_702[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_731 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_717[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1326 (Dropout)      (None, 257, 1024)            0         ['dense_731[0][0]']           \n",
      "                                                                                                  \n",
      " dense_732 (Dense)           (None, 257, 256)             262400    ['dropout_1326[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1327 (Dropout)      (None, 257, 256)             0         ['dense_732[0][0]']           \n",
      "                                                                                                  \n",
      " add_703 (Add)               (None, 257, 256)             0         ['dropout_1327[0][0]',        \n",
      "                                                                     'add_702[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_718 (L  (None, 257, 256)             512       ['add_703[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_352 (  (None, 257, 256)             3155200   ['layer_normalization_718[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_718[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_704 (Add)               (None, 257, 256)             0         ['multi_head_attention_352[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_703[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_719 (L  (None, 257, 256)             512       ['add_704[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_733 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_719[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1328 (Dropout)      (None, 257, 1024)            0         ['dense_733[0][0]']           \n",
      "                                                                                                  \n",
      " dense_734 (Dense)           (None, 257, 256)             262400    ['dropout_1328[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1329 (Dropout)      (None, 257, 256)             0         ['dense_734[0][0]']           \n",
      "                                                                                                  \n",
      " add_705 (Add)               (None, 257, 256)             0         ['dropout_1329[0][0]',        \n",
      "                                                                     'add_704[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_720 (L  (None, 257, 256)             512       ['add_705[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_353 (  (None, 257, 256)             3155200   ['layer_normalization_720[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_720[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_706 (Add)               (None, 257, 256)             0         ['multi_head_attention_353[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_705[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_721 (L  (None, 257, 256)             512       ['add_706[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_735 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_721[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1330 (Dropout)      (None, 257, 1024)            0         ['dense_735[0][0]']           \n",
      "                                                                                                  \n",
      " dense_736 (Dense)           (None, 257, 256)             262400    ['dropout_1330[0][0]']        \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_1331 (Dropout)      (None, 257, 256)             0         ['dense_736[0][0]']           \n",
      "                                                                                                  \n",
      " add_707 (Add)               (None, 257, 256)             0         ['dropout_1331[0][0]',        \n",
      "                                                                     'add_706[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_722 (L  (None, 257, 256)             512       ['add_707[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_354 (  (None, 257, 256)             3155200   ['layer_normalization_722[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_722[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_708 (Add)               (None, 257, 256)             0         ['multi_head_attention_354[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_707[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_723 (L  (None, 257, 256)             512       ['add_708[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_737 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_723[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1332 (Dropout)      (None, 257, 1024)            0         ['dense_737[0][0]']           \n",
      "                                                                                                  \n",
      " dense_738 (Dense)           (None, 257, 256)             262400    ['dropout_1332[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1333 (Dropout)      (None, 257, 256)             0         ['dense_738[0][0]']           \n",
      "                                                                                                  \n",
      " add_709 (Add)               (None, 257, 256)             0         ['dropout_1333[0][0]',        \n",
      "                                                                     'add_708[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_724 (L  (None, 257, 256)             512       ['add_709[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_355 (  (None, 257, 256)             3155200   ['layer_normalization_724[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_724[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_710 (Add)               (None, 257, 256)             0         ['multi_head_attention_355[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_709[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_725 (L  (None, 257, 256)             512       ['add_710[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_739 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_725[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1334 (Dropout)      (None, 257, 1024)            0         ['dense_739[0][0]']           \n",
      "                                                                                                  \n",
      " dense_740 (Dense)           (None, 257, 256)             262400    ['dropout_1334[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1335 (Dropout)      (None, 257, 256)             0         ['dense_740[0][0]']           \n",
      "                                                                                                  \n",
      " add_711 (Add)               (None, 257, 256)             0         ['dropout_1335[0][0]',        \n",
      "                                                                     'add_710[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_726 (L  (None, 257, 256)             512       ['add_711[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_356 (  (None, 257, 256)             3155200   ['layer_normalization_726[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_726[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_712 (Add)               (None, 257, 256)             0         ['multi_head_attention_356[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_711[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_727 (L  (None, 257, 256)             512       ['add_712[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_741 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_727[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1336 (Dropout)      (None, 257, 1024)            0         ['dense_741[0][0]']           \n",
      "                                                                                                  \n",
      " dense_742 (Dense)           (None, 257, 256)             262400    ['dropout_1336[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1337 (Dropout)      (None, 257, 256)             0         ['dense_742[0][0]']           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " add_713 (Add)               (None, 257, 256)             0         ['dropout_1337[0][0]',        \n",
      "                                                                     'add_712[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_728 (L  (None, 257, 256)             512       ['add_713[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_357 (  (None, 257, 256)             3155200   ['layer_normalization_728[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_728[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_714 (Add)               (None, 257, 256)             0         ['multi_head_attention_357[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_713[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_729 (L  (None, 257, 256)             512       ['add_714[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_743 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_729[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1338 (Dropout)      (None, 257, 1024)            0         ['dense_743[0][0]']           \n",
      "                                                                                                  \n",
      " dense_744 (Dense)           (None, 257, 256)             262400    ['dropout_1338[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1339 (Dropout)      (None, 257, 256)             0         ['dense_744[0][0]']           \n",
      "                                                                                                  \n",
      " add_715 (Add)               (None, 257, 256)             0         ['dropout_1339[0][0]',        \n",
      "                                                                     'add_714[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_730 (L  (None, 257, 256)             512       ['add_715[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_358 (  (None, 257, 256)             3155200   ['layer_normalization_730[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_730[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_716 (Add)               (None, 257, 256)             0         ['multi_head_attention_358[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_715[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_731 (L  (None, 257, 256)             512       ['add_716[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_745 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_731[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1340 (Dropout)      (None, 257, 1024)            0         ['dense_745[0][0]']           \n",
      "                                                                                                  \n",
      " dense_746 (Dense)           (None, 257, 256)             262400    ['dropout_1340[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1341 (Dropout)      (None, 257, 256)             0         ['dense_746[0][0]']           \n",
      "                                                                                                  \n",
      " add_717 (Add)               (None, 257, 256)             0         ['dropout_1341[0][0]',        \n",
      "                                                                     'add_716[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_732 (L  (None, 257, 256)             512       ['add_717[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_359 (  (None, 257, 256)             3155200   ['layer_normalization_732[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_732[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_718 (Add)               (None, 257, 256)             0         ['multi_head_attention_359[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_717[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_733 (L  (None, 257, 256)             512       ['add_718[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_747 (Dense)           (None, 257, 1024)            263168    ['layer_normalization_733[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_1342 (Dropout)      (None, 257, 1024)            0         ['dense_747[0][0]']           \n",
      "                                                                                                  \n",
      " dense_748 (Dense)           (None, 257, 256)             262400    ['dropout_1342[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1343 (Dropout)      (None, 257, 256)             0         ['dense_748[0][0]']           \n",
      "                                                                                                  \n",
      " add_719 (Add)               (None, 257, 256)             0         ['dropout_1343[0][0]',        \n",
      "                                                                     'add_718[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " layer_normalization_734 (L  (None, 257, 256)             512       ['add_719[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  (None, 256)                  0         ['layer_normalization_734[0][0\n",
      " 9 (SlicingOpLambda)                                                ]']                           \n",
      "                                                                                                  \n",
      " dense_749 (Dense)           (None, 3)                    771       ['tf.__operators__.getitem_19[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 89151235 (340.08 MB)\n",
      "Trainable params: 89151235 (340.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "class ClassToken(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
    "            trainable = True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        hidden_dim = self.w.shape[-1]\n",
    "\n",
    "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
    "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
    "        return cls\n",
    "\n",
    "def mlp(x, cf):\n",
    "    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
    "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
    "    x = Dense(cf[\"hidden_dim\"])(x)\n",
    "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
    "    return x\n",
    "\n",
    "def transformer_encoder(x, cf):\n",
    "    skip_1 = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = MultiHeadAttention(\n",
    "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
    "    )(x, x)\n",
    "    x = Add()([x, skip_1])\n",
    "\n",
    "    skip_2 = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = mlp(x, cf)\n",
    "    x = Add()([x, skip_2])\n",
    "\n",
    "    return x\n",
    "\n",
    "def ViT(cf):\n",
    "    \"\"\" Inputs \"\"\"\n",
    "    input_shape = (cf[\"num_patches\"], cf[\"patch_size\"]*cf[\"patch_size\"]*cf[\"num_channels\"])\n",
    "    inputs = Input(input_shape)     ## (None, 256, 3072)\n",
    "\n",
    "    \"\"\" Patch + Position Embeddings \"\"\"\n",
    "    patch_embed = Dense(cf[\"hidden_dim\"])(inputs)   ## (None, 256, 768)\n",
    "\n",
    "    positions = tf.range(start=0, limit=cf[\"num_patches\"], delta=1)\n",
    "    pos_embed = Embedding(input_dim=cf[\"num_patches\"], output_dim=cf[\"hidden_dim\"])(positions) ## (256, 768)\n",
    "    embed = patch_embed + pos_embed ## (None, 256, 768)\n",
    "\n",
    "    \"\"\" Adding Class Token \"\"\"\n",
    "    token = ClassToken()(embed)\n",
    "    x = Concatenate(axis=1)([token, embed]) ## (None, 257, 768)\n",
    "\n",
    "    for _ in range(cf[\"num_layers\"]):\n",
    "        x = transformer_encoder(x, cf)\n",
    "\n",
    "    \"\"\" Classification Head \"\"\"\n",
    "    x = LayerNormalization()(x)     ## (None, 257, 768)\n",
    "    x = x[:, 0, :]\n",
    "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {}\n",
    "    config[\"num_layers\"] = 24\n",
    "#     config[\"hidden_dim\"] = 768\n",
    "#     config[\"mlp_dim\"] =3072\n",
    "    config[\"num_heads\"] = 12\n",
    "    config[\"dropout_rate\"] = 0.1\n",
    "    config[\"num_patches\"] = 256\n",
    "    config[\"patch_size\"] = 32\n",
    "    config[\"num_channels\"] = 3\n",
    "    config[\"num_classes\"] = 3\n",
    "\n",
    "    config[\"hidden_dim\"] = 256\n",
    "    config[\"mlp_dim\"] = 1024  \n",
    "\n",
    "    model = ViT(config)\n",
    "    model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 63 - Valid: 21 - Test: 21\n",
      "Train: 63 - Valid: 21 - Test: 21\n",
      "Training for fold 1 ...\n",
      "2\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.7172 - acc: 0.2985 - auc: 0.4858\n",
      "Epoch 1: val_loss improved from inf to 1.73207, saving model to files/modelN_fold1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 97s 8s/step - loss: 3.7172 - acc: 0.2985 - auc: 0.4858 - val_loss: 1.7321 - val_acc: 0.1765 - val_auc: 0.7984 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 2 ...\n",
      "2\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.1043 - acc: 0.4627 - auc: 0.6007\n",
      "Epoch 1: val_loss improved from inf to 2.17573, saving model to files/modelN_fold2.h5\n",
      "5/5 [==============================] - 96s 7s/step - loss: 3.1043 - acc: 0.4627 - auc: 0.6007 - val_loss: 2.1757 - val_acc: 0.3529 - val_auc: 0.6939 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 3 ...\n",
      "2\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.2526 - acc: 0.3881 - auc: 0.5455\n",
      "Epoch 1: val_loss improved from inf to 2.94492, saving model to files/modelN_fold3.h5\n",
      "5/5 [==============================] - 91s 7s/step - loss: 3.2526 - acc: 0.3881 - auc: 0.5455 - val_loss: 2.9449 - val_acc: 0.2941 - val_auc: 0.8074 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 4 ...\n",
      "2\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.7242 - acc: 0.2836 - auc: 0.4456\n",
      "Epoch 1: val_loss improved from inf to 3.98770, saving model to files/modelN_fold4.h5\n",
      "5/5 [==============================] - 97s 8s/step - loss: 3.7242 - acc: 0.2836 - auc: 0.4456 - val_loss: 3.9877 - val_acc: 0.2941 - val_auc: 0.6280 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 5 ...\n",
      "2\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.5292 - acc: 0.3235 - auc: 0.4822WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f99ca005040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.11598, saving model to files/modelN_fold5.h5\n",
      "5/5 [==============================] - 95s 7s/step - loss: 4.5292 - acc: 0.3235 - auc: 0.4822 - val_loss: 4.1160 - val_acc: 0.2500 - val_auc: 0.5139 - lr: 1.0000e-04\n",
      "3\n",
      "Evaluating the test set...\n",
      "2/2 [==============================] - 8s 598ms/step\n",
      "2/2 [==============================] - 11s 1s/step\n",
      "2/2 [==============================] - 9s 700ms/step\n",
      "2/2 [==============================] - 8s 643ms/step\n",
      "2/2 [==============================] - 10s 725ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Yellow_rust       0.00      0.00      0.00         7\n",
      "  Brown_rust       0.38      1.00      0.55         8\n",
      "     Healthy       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.38        21\n",
      "   macro avg       0.13      0.33      0.18        21\n",
      "weighted avg       0.15      0.38      0.21        21\n",
      "\n",
      "AUC-ROC (Yellow_rust): 0.7347\n",
      "AUC-ROC (Brown_rust): 1.0000\n",
      "AUC-ROC (Healthy): 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from patchify import patchify\n",
    "# import tensorflow_hub as hub\n",
    "# Assuming you have already imported or defined the ViT model\n",
    "# ...\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "#from project import ViT\n",
    "hp = {\n",
    "    \"image_size\": 512,\n",
    "    \"num_channels\": 3,\n",
    "    \"patch_size\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"class_names\": [\"Yellow_rust\",\"Brown_rust\", \"Healthy\"],\n",
    "    \"num_layers\": 24,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"mlp_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_epochs\": 1\n",
    "}\n",
    "hp[\"num_patches\"] = (hp[\"image_size\"] ** 2) // (hp[\"patch_size\"] ** 2)\n",
    "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"] * hp[\"patch_size\"] * hp[\"num_channels\"])\n",
    "\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def load_data(path, split=0.20):\n",
    "    images = glob(os.path.join(path, \"*\", \"*.jpg\"))\n",
    "\n",
    "    # Separate images by class\n",
    "    images_class_BKL = [image for image in images if \"Yellow_rust\" in image]\n",
    "    images_class_NV = [image for image in images if \"Brown_rust\" in image]\n",
    "    images_class_MEL2 = [image for image in images if \"Healthy\" in image]\n",
    "\n",
    "    # Oversample to 33,000 to match the largest class (BKL)\n",
    "    target_size = len(images_class_BKL)\n",
    "    #print(images_class_BKL)\n",
    "    images_class_NV = np.random.choice(images_class_NV, size=target_size, replace=True).tolist()\n",
    "    images_class_MEL2 = np.random.choice(images_class_MEL2, size=target_size, replace=True).tolist()\n",
    "\n",
    "    # Concatenate lists again and shuffle\n",
    "    images = shuffle(images_class_BKL + images_class_NV + images_class_MEL2)\n",
    "\n",
    "    split_size = int(len(images) * split)\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "\n",
    "    return train_x, valid_x, test_x\n",
    "\n",
    "\n",
    "def process_image_label(path):\n",
    "    path = path.decode()\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
    "    image = image / 255.0\n",
    "\n",
    "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
    "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
    "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
    "    patches = patches.astype(np.float32)\n",
    "\n",
    "    class_name = path.split(\"/\")[-2]\n",
    "    class_idx = hp[\"class_names\"].index(class_name)\n",
    "    class_idx = np.array(class_idx, dtype=np.int32)\n",
    "    return patches, class_idx\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
    "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
    "\n",
    "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
    "    labels.set_shape(hp[\"num_classes\"])\n",
    "\n",
    "    return patches, labels\n",
    "\n",
    "\n",
    "def tf_dataset(images, batch=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = ViT(hp)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n",
    "        metrics=[\"acc\", AUC(name='auc', multi_label=True)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dir(\"files\")\n",
    "    dataset_path = '/home/ali/Music/PYTHON/DS'\n",
    "    model_path = os.path.join(\"files\", \"model3.h5\")\n",
    "    csv_path = os.path.join(\"files\", \"log2.csv\")\n",
    "    train_x, valid_x, test_x = load_data(dataset_path)\n",
    "    images = train_x + valid_x\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_no = 1\n",
    "\n",
    "    for train, val in kfold.split(images):\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        train_ds = tf_dataset(np.array(images)[train], batch=hp[\"batch_size\"])\n",
    "        valid_ds = tf_dataset(np.array(images)[val], batch=hp[\"batch_size\"])\n",
    "        model = get_model()\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(f\"files/modelN_fold{fold_no}.h5\", monitor='val_loss', verbose=1, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
    "            CSVLogger(f\"files/log2_fold{fold_no}.csv\"),\n",
    "            EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
    "        ]\n",
    "        print(2)\n",
    "        model.fit(train_ds, epochs=hp[\"num_epochs\"], validation_data=valid_ds, callbacks=callbacks)\n",
    "        fold_no += 1\n",
    "        print(3)\n",
    "\n",
    "    # Evaluating on the test set\n",
    "    print(\"Evaluating the test set...\")\n",
    "    test_ds = tf_dataset(test_x, batch=hp[\"batch_size\"])\n",
    "    all_preds = []\n",
    "    custom_objects = {\"ClassToken\": ClassToken}\n",
    "    for fold_no in range(1, 6):\n",
    "        with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "            fold_model = tf.keras.models.load_model(f\"files/modelN_fold{fold_no}.h5\")\n",
    "        y_pred_probs = fold_model.predict(test_ds)\n",
    "        all_preds.append(y_pred_probs)\n",
    "\n",
    "    avg_preds = np.mean(all_preds, axis=0)\n",
    "    final_preds = np.argmax(avg_preds, axis=1)\n",
    "    y_true = [hp[\"class_names\"].index(img.split(\"/\")[-2]) for img in test_x]\n",
    "    print(classification_report(y_true, final_preds, target_names=hp[\"class_names\"]))\n",
    "    roc_aucs = []\n",
    "\n",
    "    for i, class_name in enumerate(hp[\"class_names\"]):\n",
    "        auc_val = roc_auc_score((np.array(y_true) == i).astype(int), avg_preds[:, i])\n",
    "        roc_aucs.append(auc_val)\n",
    "        print(f\"AUC-ROC ({class_name}): {auc_val:.4f}\")\n",
    "\n",
    "    for i, class_name in enumerate(hp[\"class_names\"]):\n",
    "        fpr, tpr, _ = roc_curve((np.array(y_true) == i).astype(int), avg_preds[:, i])\n",
    "        plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_aucs[i]:.2f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2083 - Valid: 694 - Test: 694\n",
      "Train: 2083 - Valid: 694 - Test: 694\n",
      "Training for fold 1 ...\n",
      "2\n",
      "139/139 [==============================] - ETA: 0s - loss: 1.2528 - acc: 0.3679 - auc: 0.5354\n",
      "Epoch 1: val_loss improved from inf to 1.17410, saving model to files/modelN_fold1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 1017s 7s/step - loss: 1.2528 - acc: 0.3679 - auc: 0.5354 - val_loss: 1.1741 - val_acc: 0.3129 - val_auc: 0.7422 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 2 ...\n",
      "2\n",
      "139/139 [==============================] - ETA: 0s - loss: 1.2627 - acc: 0.3543 - auc: 0.5284\n",
      "Epoch 1: val_loss improved from inf to 1.02183, saving model to files/modelN_fold2.h5\n",
      "139/139 [==============================] - 1017s 7s/step - loss: 1.2627 - acc: 0.3543 - auc: 0.5284 - val_loss: 1.0218 - val_acc: 0.4227 - val_auc: 0.7840 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 3 ...\n",
      "2\n",
      "139/139 [==============================] - ETA: 0s - loss: 1.1983 - acc: 0.3947 - auc: 0.5605\n",
      "Epoch 1: val_loss improved from inf to 0.81027, saving model to files/modelN_fold3.h5\n",
      "139/139 [==============================] - 1013s 7s/step - loss: 1.1983 - acc: 0.3947 - auc: 0.5605 - val_loss: 0.8103 - val_acc: 0.6450 - val_auc: 0.8551 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 4 ...\n",
      "2\n",
      "139/139 [==============================] - ETA: 0s - loss: 1.3360 - acc: 0.3564 - auc: 0.5151\n",
      "Epoch 1: val_loss improved from inf to 1.11063, saving model to files/modelN_fold4.h5\n",
      "139/139 [==============================] - 1011s 7s/step - loss: 1.3360 - acc: 0.3564 - auc: 0.5151 - val_loss: 1.1106 - val_acc: 0.3351 - val_auc: 0.7006 - lr: 1.0000e-04\n",
      "3\n",
      "Training for fold 5 ...\n",
      "2\n",
      "139/139 [==============================] - ETA: 0s - loss: 1.3605 - acc: 0.3425 - auc: 0.5064\n",
      "Epoch 1: val_loss improved from inf to 1.07331, saving model to files/modelN_fold5.h5\n",
      "139/139 [==============================] - 1015s 7s/step - loss: 1.3605 - acc: 0.3425 - auc: 0.5064 - val_loss: 1.0733 - val_acc: 0.4577 - val_auc: 0.6791 - lr: 1.0000e-04\n",
      "3\n",
      "Evaluating the test set...\n",
      "44/44 [==============================] - 94s 2s/step\n",
      "44/44 [==============================] - 94s 2s/step\n",
      "44/44 [==============================] - 105s 2s/step\n",
      "44/44 [==============================] - 100s 2s/step\n",
      "44/44 [==============================] - 94s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Yellow_rust       0.00      0.00      0.00       218\n",
      "  Brown_rust       0.95      0.41      0.57       218\n",
      "     Healthy       0.43      1.00      0.60       258\n",
      "\n",
      "    accuracy                           0.50       694\n",
      "   macro avg       0.46      0.47      0.39       694\n",
      "weighted avg       0.46      0.50      0.40       694\n",
      "\n",
      "AUC-ROC (Yellow_rust): 0.8322\n",
      "AUC-ROC (Brown_rust): 0.8680\n",
      "AUC-ROC (Healthy): 0.8785\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from patchify import patchify\n",
    "# import tensorflow_hub as hub\n",
    "# Assuming you have already imported or defined the ViT model\n",
    "# ...\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "#from project import ViT\n",
    "hp = {\n",
    "    \"image_size\": 512,\n",
    "    \"num_channels\": 3,\n",
    "    \"patch_size\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"class_names\": [\"Yellow_rust\",\"Brown_rust\", \"Healthy\"],\n",
    "    \"num_layers\": 24,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"mlp_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_epochs\": 1\n",
    "}\n",
    "hp[\"num_patches\"] = (hp[\"image_size\"] ** 2) // (hp[\"patch_size\"] ** 2)\n",
    "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"] * hp[\"patch_size\"] * hp[\"num_channels\"])\n",
    "\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def load_data(path, split=0.20):\n",
    "    images = glob(os.path.join(path, \"*\", \"*.jpg\"))\n",
    "\n",
    "    # Separate images by class\n",
    "    images_class_BKL = [image for image in images if \"Yellow_rust\" in image]\n",
    "    images_class_NV = [image for image in images if \"Brown_rust\" in image]\n",
    "    images_class_MEL2 = [image for image in images if \"Healthy\" in image]\n",
    "\n",
    "    # Oversample to 33,000 to match the largest class (BKL)\n",
    "    target_size = len(images_class_BKL)\n",
    "    #print(images_class_BKL)\n",
    "    images_class_NV = np.random.choice(images_class_NV, size=target_size, replace=True).tolist()\n",
    "    images_class_MEL2 = np.random.choice(images_class_MEL2, size=target_size, replace=True).tolist()\n",
    "\n",
    "    # Concatenate lists again and shuffle\n",
    "    images = shuffle(images_class_BKL + images_class_NV + images_class_MEL2)\n",
    "\n",
    "    split_size = int(len(images) * split)\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "\n",
    "    return train_x, valid_x, test_x\n",
    "\n",
    "\n",
    "def process_image_label(path):\n",
    "    path = path.decode()\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
    "    image = image / 255.0\n",
    "\n",
    "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
    "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
    "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
    "    patches = patches.astype(np.float32)\n",
    "\n",
    "    class_name = path.split(\"/\")[-2]\n",
    "    class_idx = hp[\"class_names\"].index(class_name)\n",
    "    class_idx = np.array(class_idx, dtype=np.int32)\n",
    "    return patches, class_idx\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
    "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
    "\n",
    "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
    "    labels.set_shape(hp[\"num_classes\"])\n",
    "\n",
    "    return patches, labels\n",
    "\n",
    "\n",
    "def tf_dataset(images, batch=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = ViT(hp)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n",
    "        metrics=[\"acc\", AUC(name='auc', multi_label=True)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dir(\"files\")\n",
    "    dataset_path = '/home/ali/Music/PYTHON/WholeDataset'\n",
    "    model_path = os.path.join(\"files\", \"model3.h5\")\n",
    "    csv_path = os.path.join(\"files\", \"log2.csv\")\n",
    "    train_x, valid_x, test_x = load_data(dataset_path)\n",
    "    images = train_x + valid_x\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_no = 1\n",
    "\n",
    "    for train, val in kfold.split(images):\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        train_ds = tf_dataset(np.array(images)[train], batch=hp[\"batch_size\"])\n",
    "        valid_ds = tf_dataset(np.array(images)[val], batch=hp[\"batch_size\"])\n",
    "        model = get_model()\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(f\"files/modelN_fold{fold_no}.h5\", monitor='val_loss', verbose=1, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
    "            CSVLogger(f\"files/log2_fold{fold_no}.csv\"),\n",
    "            EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
    "        ]\n",
    "        print(2)\n",
    "        model.fit(train_ds, epochs=hp[\"num_epochs\"], validation_data=valid_ds, callbacks=callbacks)\n",
    "        fold_no += 1\n",
    "        print(3)\n",
    "\n",
    "    # Evaluating on the test set\n",
    "    print(\"Evaluating the test set...\")\n",
    "    test_ds = tf_dataset(test_x, batch=hp[\"batch_size\"])\n",
    "    all_preds = []\n",
    "    custom_objects = {\"ClassToken\": ClassToken}\n",
    "    for fold_no in range(1, 6):\n",
    "        with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "            fold_model = tf.keras.models.load_model(f\"files/modelN_fold{fold_no}.h5\")\n",
    "        y_pred_probs = fold_model.predict(test_ds)\n",
    "        all_preds.append(y_pred_probs)\n",
    "\n",
    "    avg_preds = np.mean(all_preds, axis=0)\n",
    "    final_preds = np.argmax(avg_preds, axis=1)\n",
    "    y_true = [hp[\"class_names\"].index(img.split(\"/\")[-2]) for img in test_x]\n",
    "    print(classification_report(y_true, final_preds, target_names=hp[\"class_names\"]))\n",
    "    roc_aucs = []\n",
    "\n",
    "    for i, class_name in enumerate(hp[\"class_names\"]):\n",
    "        auc_val = roc_auc_score((np.array(y_true) == i).astype(int), avg_preds[:, i])\n",
    "        roc_aucs.append(auc_val)\n",
    "        print(f\"AUC-ROC ({class_name}): {auc_val:.4f}\")\n",
    "\n",
    "    for i, class_name in enumerate(hp[\"class_names\"]):\n",
    "        fpr, tpr, _ = roc_curve((np.array(y_true) == i).astype(int), avg_preds[:, i])\n",
    "        plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_aucs[i]:.2f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2083 - Valid: 694 - Test: 694\n",
      "Train: 2083 - Valid: 694 - Test: 694\n",
      "Training for fold 1 ...\n",
      "2\n",
      "Epoch 1/10\n",
      "139/139 [==============================] - ETA: 0s - loss: 1.2986 - acc: 0.3516 - auc: 0.5258\n",
      "Epoch 1: val_loss improved from inf to 1.06386, saving model to files/modelN_fold1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 1005s 7s/step - loss: 1.2986 - acc: 0.3516 - auc: 0.5258 - val_loss: 1.0639 - val_acc: 0.4065 - val_auc: 0.6795 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.9857 - acc: 0.4840 - auc: 0.6872\n",
      "Epoch 2: val_loss improved from 1.06386 to 0.82759, saving model to files/modelN_fold1.h5\n",
      "139/139 [==============================] - 979s 7s/step - loss: 0.9857 - acc: 0.4840 - auc: 0.6872 - val_loss: 0.8276 - val_acc: 0.5252 - val_auc: 0.8448 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.7221 - acc: 0.6749 - auc: 0.8477\n",
      "Epoch 3: val_loss improved from 0.82759 to 0.51293, saving model to files/modelN_fold1.h5\n",
      "139/139 [==============================] - 977s 7s/step - loss: 0.7221 - acc: 0.6749 - auc: 0.8477 - val_loss: 0.5129 - val_acc: 0.8129 - val_auc: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      " 63/139 [============>.................] - ETA: 8:00 - loss: 0.6287 - acc: 0.7431 - auc: 0.8927"
     ]
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from patchify import patchify\n",
    "# import tensorflow_hub as hub\n",
    "# Assuming you have already imported or defined the ViT model\n",
    "# ...\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "#from project import ViT\n",
    "hp = {\n",
    "    \"image_size\": 512,\n",
    "    \"num_channels\": 3,\n",
    "    \"patch_size\": 64,\n",
    "    \"num_classes\": 3,\n",
    "    \"class_names\": [\"Yellow_rust\",\"Brown_rust\", \"Healthy\"],\n",
    "    \"num_layers\": 24,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"mlp_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_epochs\": 10\n",
    "}\n",
    "hp[\"num_patches\"] = (hp[\"image_size\"] ** 2) // (hp[\"patch_size\"] ** 2)\n",
    "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"] * hp[\"patch_size\"] * hp[\"num_channels\"])\n",
    "\n",
    "\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def load_data(path, split=0.20):\n",
    "    images = glob(os.path.join(path, \"*\", \"*.jpg\"))\n",
    "\n",
    "    # Separate images by class\n",
    "    images_class_BKL = [image for image in images if \"Yellow_rust\" in image]\n",
    "    images_class_NV = [image for image in images if \"Brown_rust\" in image]\n",
    "    images_class_MEL2 = [image for image in images if \"Healthy\" in image]\n",
    "\n",
    "    # Oversample to 33,000 to match the largest class (BKL)\n",
    "    target_size = len(images_class_BKL)\n",
    "    #print(images_class_BKL)\n",
    "    images_class_NV = np.random.choice(images_class_NV, size=target_size, replace=True).tolist()\n",
    "    images_class_MEL2 = np.random.choice(images_class_MEL2, size=target_size, replace=True).tolist()\n",
    "\n",
    "    # Concatenate lists again and shuffle\n",
    "    images = shuffle(images_class_BKL + images_class_NV + images_class_MEL2)\n",
    "\n",
    "    split_size = int(len(images) * split)\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "\n",
    "    return train_x, valid_x, test_x\n",
    "\n",
    "\n",
    "def process_image_label(path):\n",
    "    path = path.decode()\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
    "    image = image / 255.0\n",
    "\n",
    "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
    "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
    "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
    "    patches = patches.astype(np.float32)\n",
    "\n",
    "    class_name = path.split(\"/\")[-2]\n",
    "    class_idx = hp[\"class_names\"].index(class_name)\n",
    "    class_idx = np.array(class_idx, dtype=np.int32)\n",
    "    return patches, class_idx\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
    "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
    "\n",
    "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
    "    labels.set_shape(hp[\"num_classes\"])\n",
    "\n",
    "    return patches, labels\n",
    "\n",
    "\n",
    "def tf_dataset(images, batch=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = ViT(hp)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(hp[\"lr\"], clipvalue=1.0),\n",
    "        metrics=[\"acc\", AUC(name='auc', multi_label=True)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dir(\"files\")\n",
    "    dataset_path = '/home/ali/Music/PYTHON/WholeDataset'\n",
    "    model_path = os.path.join(\"files\", \"model3.h5\")\n",
    "    csv_path = os.path.join(\"files\", \"log2.csv\")\n",
    "    train_x, valid_x, test_x = load_data(dataset_path)\n",
    "    images = train_x + valid_x\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_no = 1\n",
    "\n",
    "    for train, val in kfold.split(images):\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        train_ds = tf_dataset(np.array(images)[train], batch=hp[\"batch_size\"])\n",
    "        valid_ds = tf_dataset(np.array(images)[val], batch=hp[\"batch_size\"])\n",
    "        model = get_model()\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(f\"files/modelN_fold{fold_no}.h5\", monitor='val_loss', verbose=1, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
    "            CSVLogger(f\"files/log2_fold{fold_no}.csv\"),\n",
    "            EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
    "        ]\n",
    "        print(2)\n",
    "        model.fit(train_ds, epochs=hp[\"num_epochs\"], validation_data=valid_ds, callbacks=callbacks)\n",
    "        fold_no += 1\n",
    "        print(3)\n",
    "\n",
    "    # Evaluating on the test set\n",
    "    print(\"Evaluating the test set...\")\n",
    "    test_ds = tf_dataset(test_x, batch=hp[\"batch_size\"])\n",
    "    all_preds = []\n",
    "    custom_objects = {\"ClassToken\": ClassToken}\n",
    "    for fold_no in range(1, 6):\n",
    "        with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "            fold_model = tf.keras.models.load_model(f\"files/modelN_fold{fold_no}.h5\")\n",
    "        y_pred_probs = fold_model.predict(test_ds)\n",
    "        all_preds.append(y_pred_probs)\n",
    "\n",
    "    avg_preds = np.mean(all_preds, axis=0)\n",
    "    final_preds = np.argmax(avg_preds, axis=1)\n",
    "    y_true = [hp[\"class_names\"].index(img.split(\"/\")[-2]) for img in test_x]\n",
    "    print(classification_report(y_true, final_preds, target_names=hp[\"class_names\"]))\n",
    "    roc_aucs = []\n",
    "\n",
    "    for i, class_name in enumerate(hp[\"class_names\"]):\n",
    "        auc_val = roc_auc_score((np.array(y_true) == i).astype(int), avg_preds[:, i])\n",
    "        roc_aucs.append(auc_val)\n",
    "        print(f\"AUC-ROC ({class_name}): {auc_val:.4f}\")\n",
    "\n",
    "    for i, class_name in enumerate(hp[\"class_names\"]):\n",
    "        fpr, tpr, _ = roc_curve((np.array(y_true) == i).astype(int), avg_preds[:, i])\n",
    "        plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {roc_aucs[i]:.2f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
